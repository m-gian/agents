{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-gian/agents/blob/main/Create_Agent_from_scratch_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Google GenAI library for interacting with Google's generative AI models.\n",
        "!pip install -U -q google-genai"
      ],
      "metadata": {
        "id": "9Btp1x9hhQGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for file operations, JSON handling, unique IDs, timing, and data structures.\n",
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime, UTC\n",
        "from typing import Dict, List, Callable, Any, Literal, Optional, Union\n",
        "# Import Google GenAI types and client for AI model interaction.\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "# Import Pydantic for data validation and settings management.\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "dOAVtTv6Y_Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import `userdata` from `google.colab` to securely access user-defined secrets.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the Gemini API key from Colab's user data secrets.\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize the GenAI client with the retrieved API key.\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "uHUqGrPuhmwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an `AgentObserver` class to log events and trace the agent's execution.\n",
        "class AgentObserver:\n",
        "    def __init__(self, log_dir=\"/content/logs\"):\n",
        "        # Generate a unique trace ID for each run.\n",
        "        self.trace_id = str(uuid.uuid4())\n",
        "        # Store a list of events during the trace.\n",
        "        self.events = []\n",
        "\n",
        "        # Create the log directory if it doesn't exist.\n",
        "        Path(log_dir).mkdir(exist_ok=True)\n",
        "        # Define the path for the log file.\n",
        "        self.file_path = Path(log_dir) / f\"trace_{self.trace_id}.jsonl\"\n",
        "\n",
        "    def log(self, event_type, data=None):\n",
        "        # Create a log entry with timestamp, event type, and data.\n",
        "        entry = {\n",
        "            \"trace_id\": self.trace_id,\n",
        "            \"timestamp\": time.time(),\n",
        "            \"event\": event_type,\n",
        "            \"data\": data or {}\n",
        "        }\n",
        "\n",
        "        # Append the entry to the in-memory events list.\n",
        "        self.events.append(entry)\n",
        "\n",
        "        # Write the log entry to the JSONL file.\n",
        "        with open(self.file_path, \"a\") as f:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "    def span(self, name):\n",
        "        # Create a new `Span` object for timing specific operations.\n",
        "        return Span(self, name)\n",
        "\n",
        "# Define a `Span` class for measuring the duration of operations (context manager).\n",
        "class Span:\n",
        "    def __init__(self, observer, name):\n",
        "        self.observer = observer\n",
        "        self.name = name\n",
        "\n",
        "    def __enter__(self):\n",
        "        # Record the start time and log a 'span_start' event.\n",
        "        self.start = time.time()\n",
        "        self.observer.log(\"span_start\", {\"name\": self.name})\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        # Calculate the duration and log a 'span_end' event.\n",
        "        duration = time.time() - self.start\n",
        "        self.observer.log(\"span_end\", {\n",
        "            \"name\": self.name,\n",
        "            \"duration_sec\": round(duration, 3)\n",
        "        })"
      ],
      "metadata": {
        "id": "T10ZJYRJ2Egl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a `MemoryStore` class for persistent storage of conversational memory.\n",
        "class MemoryStore:\n",
        "    def __init__(self, file_path: str, max_entries: int = 50):\n",
        "        self.file_path = file_path\n",
        "        self.max_entries = max_entries\n",
        "        # Ensure the memory file exists upon initialization.\n",
        "        self._ensure_file()\n",
        "\n",
        "    def _ensure_file(self):\n",
        "        # Create an empty JSON array in the file if it doesn't exist.\n",
        "        if not os.path.exists(self.file_path):\n",
        "            with open(self.file_path, \"w\") as f:\n",
        "                json.dump([], f)\n",
        "\n",
        "    def load_all(self) -> List[dict]:\n",
        "        # Load all entries from the memory file.\n",
        "        try:\n",
        "            with open(self.file_path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception:\n",
        "            # Return an empty list if the file is not found or corrupted.\n",
        "            return []\n",
        "\n",
        "    def append(self, entry: dict):\n",
        "        # Append a new entry to the memory file.\n",
        "        data = self.load_all()\n",
        "        data.append(entry)\n",
        "\n",
        "        with open(self.file_path, \"w\") as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "\n",
        "    def get_recent(self, limit: Optional[int] = None) -> list[dict]:\n",
        "        # Retrieve a limited number of recent entries from memory.\n",
        "        data = self.load_all()\n",
        "        limit = limit or self.max_entries\n",
        "        return data[-limit:]\n",
        "\n",
        "    def delete_all(self):\n",
        "        # Clear all entries from the memory file.\n",
        "        with open(self.file_path, \"w\") as f:\n",
        "            json.dump([], f)"
      ],
      "metadata": {
        "id": "Ubxbob80HqqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the `MemoryStore` for the agent's long-term memory.\n",
        "memory_store = MemoryStore(\n",
        "    file_path=\"/content/agent_memory.json\", # Path to the memory file.\n",
        "    max_entries=10, # Maximum number of entries to retain.\n",
        ")"
      ],
      "metadata": {
        "id": "xIckz7Dn4ZTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a `Tool` class to represent an external function callable by the agent.\n",
        "class Tool:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        description: str,\n",
        "        input_schema: Dict[str, Any],\n",
        "        output_schema: Dict[str, Any],\n",
        "        func: Callable[..., Any],\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.input_schema = input_schema\n",
        "        self.output_schema = output_schema\n",
        "        self.func = func\n",
        "\n",
        "    def __call__(self, **kwargs):\n",
        "        # Allow the tool to be called like a function.\n",
        "        return self.func(**kwargs)"
      ],
      "metadata": {
        "id": "d2rmAnxaQyZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a `ToolRegistry` class to manage and provide access to available tools.\n",
        "class ToolRegistry:\n",
        "    def __init__(self):\n",
        "        self.tools: Dict[str, Tool] = {}\n",
        "\n",
        "    def register(self, tool: Tool):\n",
        "        # Add a tool to the registry.\n",
        "        self.tools[tool.name] = tool\n",
        "\n",
        "    def get(self, name: str) -> Tool:\n",
        "        # Retrieve a tool by its name.\n",
        "        if name not in self.tools.keys():\n",
        "            raise ValueError(f\"Tool '{name}' not found\")\n",
        "        return self.tools[name]\n",
        "\n",
        "    def list_tools(self) -> List[Dict[str, Any]]:\n",
        "        # Return a list of tool descriptions, including their input schemas.\n",
        "        return [\n",
        "            {\n",
        "                \"name\": tool.name,\n",
        "                \"description\": tool.description,\n",
        "                \"input_schema\": tool.input_schema.model_json_schema(),\n",
        "            }\n",
        "            for tool in self.tools.values()\n",
        "        ]\n",
        "\n",
        "    def get_tool_call_args_type(self) -> Union[BaseModel]:\n",
        "        # Generate a Pydantic Union type for all tool input arguments.\n",
        "        input_args_models = [tool.input_schema for tool in self.tools.values()]\n",
        "        tool_call_args = Union[tuple(input_args_models)]\n",
        "        return tool_call_args\n",
        "\n",
        "    def get_tool_names(self) -> Literal[None]:\n",
        "        # Return a Literal type containing all registered tool names.\n",
        "        return Literal[*self.tools.keys()]"
      ],
      "metadata": {
        "id": "3glc8fGVfju5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define simple mathematical functions that can be used as tools.\n",
        "def add(a: int, b: int) -> int:\n",
        "    return a + b\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    return a * b\n",
        "\n",
        "# Factory function to create a tool for deleting all memory.\n",
        "def make_delete_all_memory_tool(memory_store: MemoryStore):\n",
        "    def delete_all_memory(confirm: str):\n",
        "        print(confirm)\n",
        "        # Require explicit confirmation to prevent accidental memory deletion.\n",
        "        if confirm.lower() != \"true\":\n",
        "            raise ValueError(\n",
        "                \"delete_all_memory called without explicit confirmation\"\n",
        "            )\n",
        "\n",
        "        memory_store.delete_all()\n",
        "        return \"All long-term memory has been permanently deleted.\"\n",
        "\n",
        "    return delete_all_memory\n",
        "\n",
        "# Create an instance of the delete_all_memory function using the memory_store.\n",
        "delete_all_memory_fn = make_delete_all_memory_tool(memory_store)"
      ],
      "metadata": {
        "id": "8jOT8dPbx05k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the `ToolRegistry`.\n",
        "registry = ToolRegistry()\n",
        "\n",
        "# Define Pydantic models for the input arguments of each tool.\n",
        "class ToolAddArgs(BaseModel):\n",
        "    a: int\n",
        "    b: int\n",
        "\n",
        "class ToolMultiplyArgs(BaseModel):\n",
        "    a: int\n",
        "    b: int\n",
        "\n",
        "class DeleteAllMemoryArgs(BaseModel):\n",
        "    confirm: Literal[\"true\"] = Field(\n",
        "        description=\"Must be 'true' to confirm permanent deletion of all memory.\"\n",
        "    )\n",
        "\n",
        "# Register the 'add' tool with its description, input/output schemas, and function.\n",
        "registry.register(\n",
        "    Tool(\n",
        "        name=\"add\",\n",
        "        description=\"Add two numbers\",\n",
        "        input_schema=ToolAddArgs,\n",
        "        output_schema={\"result\": \"int\"},\n",
        "        func=add,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Register the 'multiply' tool.\n",
        "registry.register(\n",
        "    Tool(\n",
        "        name=\"multiply\",\n",
        "        description=\"Multiply two numbers\",\n",
        "        input_schema=ToolMultiplyArgs,\n",
        "        output_schema={\"result\": \"int\"},\n",
        "        func=multiply,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Register the 'delete_all_memory' tool.\n",
        "registry.register(\n",
        "    Tool(\n",
        "        name=\"delete_all_memory\",\n",
        "        description=\"Permanently delete all long-term memory. This action is irreversible.\",\n",
        "        input_schema=DeleteAllMemoryArgs,\n",
        "        output_schema={\"result\": \"string\"},\n",
        "        func=delete_all_memory_fn,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "k486HLyDx024"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a Literal type of all registered tool names for type hinting.\n",
        "ToolNameLiteral = registry.get_tool_names()\n",
        "# Get a Union type of all tool argument schemas for type hinting.\n",
        "ToolArgsUnion = registry.get_tool_call_args_type()\n",
        "\n",
        "# Define Pydantic models for different types of LLM responses.\n",
        "class ToolCall(BaseModel):\n",
        "    action: Literal[\"tool\"]\n",
        "    thought: str\n",
        "    tool_name: ToolNameLiteral\n",
        "    args: ToolArgsUnion\n",
        "\n",
        "class FinalAnswer(BaseModel):\n",
        "    action: Literal[\"final\"]\n",
        "    answer: str\n",
        "\n",
        "class HumanApproval(BaseModel):\n",
        "    action: Literal[\"human\"]\n",
        "    reason: str\n",
        "\n",
        "# Define a Union type for all possible LLM responses.\n",
        "LLMResponse = Union[ToolCall, FinalAnswer, HumanApproval]\n",
        "\n",
        "# Define the `GeminiLLM` class to interact with the Google Gemini model.\n",
        "class GeminiLLM:\n",
        "    def __init__(self, client, tool_registry, model=\"gemini-2.5-flash\"):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.tool_registry = tool_registry\n",
        "        # Generate the system instruction for the LLM based on available tools.\n",
        "        self.system_instruction = self._create_system_instruction()\n",
        "\n",
        "    def _create_system_instruction(self) -> str:\n",
        "        # Format tool descriptions into a JSON string for the system prompt.\n",
        "        tools_description = json.dumps(\n",
        "            self.tool_registry.list_tools(),\n",
        "            indent=2\n",
        "        )\n",
        "\n",
        "        # Construct the detailed system prompt with rules and tool information.\n",
        "        system_prompt = \"\"\"\n",
        "You are a conversational AI agent that can interact with external tools.\n",
        "\n",
        "CRITICAL RULES (MUST FOLLOW):\n",
        "- You are NOT allowed to perform actions internally if a tool is provided in the registry for that action.\n",
        "- If a tool exists that can perform any part of the task, you MUST use that tool.\n",
        "- You MUST NOT skip tools, even for simple or obvious steps.\n",
        "- You MUST NOT combine multiple operations into a single step unless a tool explicitly supports it.\n",
        "- You may ONLY produce a final answer when no available tool can further advance the task.\n",
        "\n",
        "HUMAN-IN-THE-LOOP (MANDATORY):\n",
        "- You have a special action called \"human\".\n",
        "- You MUST choose the \"human\" action BEFORE performing any irreversible, destructive, or sensitive operation.\n",
        "- Examples include (but are not limited to): deleting memory, resetting state, or permanently altering stored data.\n",
        "- When using the \"human\" action, you MUST clearly explain the reason approval is required.\n",
        "- After asking for human approval, you have two options depending on the response:\n",
        "    1. In case the approval is given: You MUST continue the task by selecting the appropriate next action (usually a tool call).\n",
        "    2. In case the approval is denied: You MUST continue the conversation, informing the use that the original action won't be perfomed because\n",
        "    approval was not given.\n",
        "- Do not repeat this action consecutively. You must always follow a \"human\" action by a \"tool\" action.\n",
        "\n",
        "IMPORTANT CLARIFICATION:\n",
        "- Internal reasoning, text parsing, extraction, summarization, and analysis of the provided input\n",
        "  are NOT considered tool-eligible operations.\n",
        "- These cognitive operations MUST be performed internally by the model.\n",
        "- Tools are ONLY required for external actions, side effects, or interactions with systems\n",
        "  outside the model (APIs, databases, files, network, state mutation).\n",
        "\n",
        "TOOL USAGE RULES:\n",
        "- Each tool call must perform exactly ONE meaningful operation.\n",
        "- If the task requires multiple operations, you MUST call tools sequentially.\n",
        "- If multiple tools could apply, choose the most specific one.\n",
        "- Tools MUST NOT be called unless explicitly selected as an action.\n",
        "\n",
        "RESPONSE FORMAT (STRICT):\n",
        "- You MUST respond ONLY in valid JSON.\n",
        "- Never include explanations outside JSON.\n",
        "- You must choose exactly ONE action per response.\n",
        "\n",
        "Allowed actions:\n",
        "1. Tool call:\n",
        "{\n",
        "\"action\": \"tool\",\n",
        "\"thought\": \"...\",\n",
        "\"tool_name\": \"...\",\n",
        "\"inputs\": { ... }\n",
        "}\n",
        "\n",
        "2. Human approval request:\n",
        "{\n",
        "\"action\": \"human\",\n",
        "\"reason\": \"Clear explanation of why human approval is required.\"\n",
        "}\n",
        "\n",
        "3. Final answer (only if no tool or approval is needed):\n",
        "{\n",
        "\"action\": \"final\",\n",
        "\"answer\": \"...\"\n",
        "}\n",
        "\"\"\" + \"\\n\\nAvailable tools in the registry with description:\\n\" + tools_description\n",
        "        return system_prompt\n",
        "\n",
        "\n",
        "    def _format_gemini_chat_history(self, history: list[dict]) -> list:\n",
        "        # Convert the internal chat history format to Gemini's expected format.\n",
        "        formatted_history = []\n",
        "        for message in history:\n",
        "            if message[\"role\"] == \"user\":\n",
        "                formatted_history.append(types.Content(\n",
        "                        role=\"user\",\n",
        "                        parts=[\n",
        "                            types.Part.from_text(text=message[\"content\"])\n",
        "                        ]\n",
        "                    )\n",
        "                )\n",
        "            if message[\"role\"] == \"assistant\":\n",
        "                formatted_history.append(types.Content(\n",
        "                        role=\"model\",\n",
        "                        parts=[\n",
        "                            types.Part.from_text(text=message[\"content\"])\n",
        "                        ]\n",
        "                    )\n",
        "                )\n",
        "            if message[\"role\"] == \"tool\":\n",
        "                formatted_history.append(types.Content(\n",
        "                        role=\"tool\",\n",
        "                        parts=[\n",
        "                            types.Part.from_function_response(\n",
        "                                name=message[\"tool_name\"],\n",
        "                                response={'result': message[\"tool_response\"]},\n",
        "                            )\n",
        "                        ]\n",
        "                    )\n",
        "                )\n",
        "        return formatted_history\n",
        "\n",
        "\n",
        "    def generate(self, history: list[dict]) -> str:\n",
        "        # Generate content using the Gemini model.\n",
        "        gemini_history_format = self._format_gemini_chat_history(history)\n",
        "        #print(gemini_history_format)\n",
        "        response = self.client.models.generate_content(\n",
        "            model=self.model,\n",
        "            contents=gemini_history_format,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0,\n",
        "                response_mime_type=\"application/json\",\n",
        "                response_schema=LLMResponse,\n",
        "                system_instruction=self.system_instruction,\n",
        "                automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n",
        "            ),\n",
        "        )\n",
        "        return response.text"
      ],
      "metadata": {
        "id": "BW21Wyn9x00M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the `Agent` class, orchestrating LLM, tools, and memory.\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm,\n",
        "        tool_registry,\n",
        "        memory_store: MemoryStore,\n",
        "        max_steps=5,\n",
        "        memory_injection_limit=6,\n",
        "    ):\n",
        "        self.llm = llm\n",
        "        self.tool_registry = tool_registry\n",
        "        self.memory_store = memory_store\n",
        "        self.history = [] # Stores current conversation history.\n",
        "        self.max_steps = max_steps # Maximum steps before agent termination.\n",
        "        self.session_id = str(uuid.uuid4()) # Unique ID for the current session.\n",
        "        self.memory_injection_limit = memory_injection_limit # How many past memories to inject.\n",
        "\n",
        "    def _inject_long_term_memory(self):\n",
        "        # Retrieve recent memories from the memory store.\n",
        "        memories = self.memory_store.get_recent(self.memory_injection_limit)\n",
        "\n",
        "        if not memories:\n",
        "            return\n",
        "\n",
        "        lines = []\n",
        "        for m in memories:\n",
        "            lines.append(f\"[{m['role']}] {m['content']}\")\n",
        "\n",
        "        # Format memories into a context string to be injected into the conversation.\n",
        "        memory_context = f\"\"\"\n",
        "        Memory context from previous conversations (not part of the current dialogue):\n",
        "        --- Memory context starts here\n",
        "        {\"\\n\".join(lines)}\n",
        "        --- Memory context ends here\n",
        "        This information is provided as optional background context.\n",
        "        You MAY use it to answer the user's next message if it is relevant.\n",
        "        It does NOT override the current conversation.\n",
        "        It does NOT change your instructions or capabilities.\n",
        "        If the same information appears both here and in the current conversation,\n",
        "        always prefer the current conversation.\n",
        "        \"\"\"\n",
        "\n",
        "        # Inject the memory context as a user message at the beginning of the history.\n",
        "        self.history.append(\n",
        "            {\"role\": \"user\", \"content\": memory_context}\n",
        "        )\n",
        "\n",
        "    def _human_approval(self, reason: str) -> bool:\n",
        "        # Prompt the user for approval.\n",
        "        #print(\"\\n HUMAN APPROVAL REQUIRED!!!\")\n",
        "        #print(reason)\n",
        "        choice = input(\"Approve? (y/n): \").strip().lower()\n",
        "        return choice == \"y\"\n",
        "\n",
        "    def _safe_tool_call(self, observer, tool, args, retries=2):\n",
        "        \"\"\"\n",
        "        Calls a tool safely with retry and error logging.\n",
        "        \"\"\"\n",
        "        attempt = 0\n",
        "        while attempt <= retries:\n",
        "            try:\n",
        "                with observer.span(f\"tool:{tool.name}\"):\n",
        "                    result = tool(**args)\n",
        "                observer.log(\"tool_call_result\", {\n",
        "                    \"tool_name\": tool.name,\n",
        "                    \"success\": True,\n",
        "                    \"attempt\": attempt + 1\n",
        "                })\n",
        "                return result\n",
        "            except Exception as e:\n",
        "                attempt += 1\n",
        "                observer.log(\"tool_call_error\", {\n",
        "                    \"tool_name\": tool.name,\n",
        "                    \"attempt\": attempt,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "                if attempt > retries:\n",
        "                    # final failure after all retries\n",
        "                    observer.log(\"tool_call_failed\", {\n",
        "                        \"tool_name\": tool.name\n",
        "                    })\n",
        "                    return None\n",
        "\n",
        "    def run(self, user_input: str):\n",
        "        # Initialize an observer for logging agent actions.\n",
        "        observer = AgentObserver()\n",
        "        observer.log(\"run_start\", {\n",
        "            \"session_id\": self.session_id\n",
        "        })\n",
        "\n",
        "        # Inject long-term memory at the start of a new conversation.\n",
        "        if not self.history:\n",
        "            self._inject_long_term_memory()\n",
        "\n",
        "        # Add the user's input to the conversation history.\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        observer.log(\"user_message\", {\n",
        "            \"text\": user_input\n",
        "        })\n",
        "\n",
        "        # Main agent loop for processing steps.\n",
        "        for step in range(self.max_steps):\n",
        "            with observer.span(\"llm_call\"):\n",
        "                # Generate LLM output based on current history.\n",
        "                llm_output = self.llm.generate(self.history)\n",
        "            # Parse the LLM's action from the JSON output.\n",
        "            action = json.loads(llm_output)\n",
        "            observer.log(\"llm_decision\", {\n",
        "                \"step\": step,\n",
        "                \"action\": action[\"action\"]\n",
        "            })\n",
        "\n",
        "            # Handle 'human' action for approval.\n",
        "            if action[\"action\"] == \"human\":\n",
        "                observer.log(\"human_approval_requested\", {\n",
        "                    \"reason\": action[\"reason\"]\n",
        "                })\n",
        "                self.history.append(\n",
        "                    {\"role\": \"assistant\", \"content\": action[\"reason\"]}\n",
        "                )\n",
        "                approved = self._human_approval(action[\"reason\"])\n",
        "                observer.log(\"human_approval_result\", {\n",
        "                    \"approved\": approved\n",
        "                })\n",
        "\n",
        "                if not approved:\n",
        "                    # Inform the agent if approval is denied.\n",
        "                    self.history.append({\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"Human approval was demanded and it is not given. You can not perform the action that required the approval.\"\n",
        "                    })\n",
        "\n",
        "                # Inform the agent if approval is given.\n",
        "                self.history.append({\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Human approval was demanded and it is given. You can now proceed with the action that required the approval.\"\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Handle 'tool' action for tool execution.\n",
        "            if action[\"action\"] == \"tool\":\n",
        "                observer.log(\"tool_call_requested\", {\n",
        "                    \"tool_name\": action[\"tool_name\"],\n",
        "                    \"args\": action[\"args\"]\n",
        "                })\n",
        "                self.history.append(\n",
        "                    {\"role\": \"assistant\", \"content\": llm_output}\n",
        "                )\n",
        "                # Get the tool from the registry and execute it.\n",
        "                tool = self.tool_registry.get(action[\"tool_name\"])\n",
        "                result = self._safe_tool_call(observer, tool, action[\"args\"])\n",
        "                observer.log(\"tool_call_result\", {\n",
        "                    \"tool_name\": tool.name,\n",
        "                    \"tool_response\": result,\n",
        "                })\n",
        "\n",
        "                # Special handling for memory deletion to clear history.\n",
        "                if action[\"tool_name\"] == \"delete_all_memory\":\n",
        "                    observer.log(\"memory_cleared\")\n",
        "                    self.history = self.history[1:] # Clear chat history after memory deletion\n",
        "\n",
        "                # Add tool result to history.\n",
        "                self.history.append(\n",
        "                    {\n",
        "                        \"role\": \"tool\",\n",
        "                        \"tool_name\": tool.name,\n",
        "                        \"tool_response\": result,\n",
        "                    }\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            # Handle 'final' action to conclude the conversation.\n",
        "            if action[\"action\"] == \"final\":\n",
        "                self.history.append(\n",
        "                    {\"role\": \"assistant\", \"content\": llm_output}\n",
        "                )\n",
        "                observer.log(\"final_answer\", {\n",
        "                    \"text\": action[\"answer\"]\n",
        "                })\n",
        "\n",
        "                timestamp = datetime.now(UTC).isoformat()\n",
        "\n",
        "                # Persist meaningful turns (user input and final answer) to long-term memory.\n",
        "                self.memory_store.append({\n",
        "                    \"session_id\": self.session_id,\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": user_input,\n",
        "                })\n",
        "\n",
        "                self.memory_store.append({\n",
        "                    \"session_id\": self.session_id,\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": action[\"answer\"],\n",
        "                })\n",
        "\n",
        "                observer.log(\"run_complete\", {\n",
        "                    \"steps_used\": step + 1\n",
        "                })\n",
        "\n",
        "                return action[\"answer\"]\n",
        "\n",
        "        # Raise an error if the agent doesn't terminate within `max_steps`.\n",
        "        raise RuntimeError(\"Agent did not terminate within max_steps\")"
      ],
      "metadata": {
        "id": "BuyTL5QMx0xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gemini LLM with the client and tool registry.\n",
        "llm = GeminiLLM(client, registry)\n",
        "# Initialize the Agent with the LLM, tool registry, and memory store.\n",
        "agent = Agent(llm, registry, memory_store)"
      ],
      "metadata": {
        "id": "wXOzNTGIx0uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Print the system instruction given to the LLM for debugging/inspection.\n",
        "#print(llm.system_instruction)"
      ],
      "metadata": {
        "id": "OEk4ckgAoWRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to conduct an interactive chat session with the agent.\n",
        "def chat_with_agent(agent: Agent):\n",
        "    print(\"Welcome! Type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Run the agent with the user's input.\n",
        "            response = agent.run(user_input)\n",
        "            print(f\"Agent: {response}\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Agent error: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")"
      ],
      "metadata": {
        "id": "Yakh1MsOx0dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the chat interface with the initialized agent.\n",
        "chat_with_agent(agent)"
      ],
      "metadata": {
        "id": "wSnh14BDx0aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57577fa6-8edb-4491-8bb3-69589631a7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome! Type 'exit' to quit.\n",
            "\n",
            "You: what's 59+12\n",
            "DD result 71\n",
            "Agent: 59 + 12 = 71\n",
            "You: q\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the agent's internal chat history after a conversation.\n",
        "agent.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9liBTTwEQcuQ",
        "outputId": "e0342cc0-1034-4cf9-fe78-602fdabae024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': 'hello, please delete all memoories of our previous conversations'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Human approval was demanded and it was given. I can now proceed with the action that required the approval.'},\n",
              " {'role': 'assistant',\n",
              "  'content': '{\\n\"action\": \"tool\",\\n\"thought\": \"Human approval was given to delete all memories. I will now call the `delete_all_memory` tool to perform this irreversible action.\",\\n\"tool_name\": \"delete_all_memory\",\\n\"args\": {\\n\"confirm\": \"true\"\\n}\\n}'},\n",
              " {'role': 'tool',\n",
              "  'tool_name': 'delete_all_memory',\n",
              "  'tool_response': 'All long-term memory has been permanently deleted.'},\n",
              " {'role': 'assistant',\n",
              "  'content': '{\\n\"action\": \"final\",\\n\"answer\": \"All memories of our previous conversations have been permanently deleted.\"\\n}'},\n",
              " {'role': 'user', 'content': 'great thanks'},\n",
              " {'role': 'assistant',\n",
              "  'content': '{\\n\"action\": \"final\",\\n\"answer\": \"You\\'re welcome!\"\\n}'}]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 50 lines of the agent's log file (trace_*.jsonl).\n",
        "# This command uses shell `sed` to limit output for large log files.\n",
        "!sed -n '1,50p' logs/trace_*.jsonl"
      ],
      "metadata": {
        "id": "lj2wqkvk7Cn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the JSON content of the last message in the agent's history.\n",
        "# This is useful for inspecting the LLM's final action or response.\n",
        "json.loads(agent.history[-1][\"content\"])"
      ],
      "metadata": {
        "id": "_YaQePiKx0WT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98f0826-407c-47f1-c709-21edb0025788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'action': 'final', 'answer': \"I'm glad you think so!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually call the LLM to generate a response for a specific chat history.\n",
        "# This bypasses the full agent run loop and is useful for testing LLM behavior in isolation.\n",
        "r = llm.generate([{'role': 'user', 'content': 'hi, Can you add 10 and 32?'},\n",
        " {'role': 'assistant',\n",
        "  'content': '{\\n  \"action\": \"tool\",\\n  \"thought\": \"The user wants to add two numbers, 10 and 32. I should use the \\'add\\' tool for this.\",\\n  \"tool_name\": \"add\",\\n  \"args\": {\\n    \"a\": 10,\\n    \"b\": 32\\n  }\\n}'},\n",
        " {'role': 'tool', 'tool_name': 'add', 'tool_response': 42},\n",
        " {'role': 'assistant',\n",
        "  'content': '{\\n  \"action\": \"final\",\\n  \"answer\": \"42\"\\n}'},\n",
        " {'role': 'user', 'content': 'now multiply that by 2'}] )"
      ],
      "metadata": {
        "id": "AACWiqprx0T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the raw response from the LLM generated in the previous cell.\n",
        "r"
      ],
      "metadata": {
        "id": "Y2z7sIQ5ubxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}